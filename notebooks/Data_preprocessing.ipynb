{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlLi3FVWDWhDZi/fldDIzZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Set Up"],"metadata":{"id":"AS_iszu_QgMt"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnAXSzVxQVZy","executionInfo":{"status":"ok","timestamp":1686782228007,"user_tz":-120,"elapsed":18302,"user":{"displayName":"Chang YenYu","userId":"13353243669334155647"}},"outputId":"d5523f73-1e93-44f5-a2d5-88288928414b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import math\n","!pip install tweet-preprocessor"],"metadata":{"id":"6R2YzRayTu12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686782233227,"user_tz":-120,"elapsed":5223,"user":{"displayName":"Chang YenYu","userId":"13353243669334155647"}},"outputId":"a3c45159-48e3-4a8f-f896-0b8dd36aa9a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n"]}]},{"cell_type":"markdown","source":["# Data\n","\n","For our training, we have a few corpora to work with.\n","1. the CONAN datasets\n","2. the QIAN Benchmark dataset\n","\n","First we read our corpora as raw dataframes with pandas.\n","\n","---\n","\n"],"metadata":{"id":"0DrWHIChQofZ"}},{"cell_type":"code","source":["# set data paths\n","\n","root_dir = \"gdrive/My Drive/Master_Thesis/\"\n","data_dir = os.path.join(root_dir, 'data/')\n","Qian_gab_dir = os.path.join(data_dir, 'Qian/gab.csv')\n","Qian_reddit_dir = os.path.join(data_dir, 'Qian/reddit.csv')\n","CONAN_dir = os.path.join(data_dir, 'CONAN/CONAN.csv')\n","Multitarget_CONAN_dir = os.path.join(data_dir, 'Multitarget-CONAN/Multitarget-CONAN.csv')\n","DIALO_CONAN_dir = os.path.join(data_dir, 'DIALOCONAN/DIALOCONAN.csv')\n","KN_CONAN_dir = os.path.join(data_dir, 'multitarget_KN_grounded_CN/multitarget_KN_grounded_CN.csv')"],"metadata":{"id":"30n421LTQsVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read csv file into dataframe\n","\n","raw_CONAN = pd.read_csv(CONAN_dir)\n","raw_MultiCONAN = pd.read_csv(Multitarget_CONAN_dir)\n","raw_DIALOCONAN = pd.read_csv(DIALO_CONAN_dir)\n","raw_KNCONAN = pd.read_csv(KN_CONAN_dir)\n","\n","raw_Qian_gab = pd.read_csv(Qian_gab_dir)\n","raw_Qian_reddit = pd.read_csv(Qian_reddit_dir)"],"metadata":{"id":"TbMg1HhIQtWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_Qian_gab.loc[0]['text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"PBDsymnNDqNV","executionInfo":{"status":"ok","timestamp":1686782615724,"user_tz":-120,"elapsed":287,"user":{"displayName":"Chang YenYu","userId":"13353243669334155647"}},"outputId":"04afd45d-5a3d-414d-c5b3-16fef427e5d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["## Sexist Hate Speech dataset"],"metadata":{"id":"WVhp5bfNuzFe"}},{"cell_type":"code","source":["EDOS_dir = os.path.join(data_dir, 'EDOS/edos_labelled_aggregated.csv')\n","SBIC_train_dir = os.path.join(data_dir, 'SBIC/SBIC.v2.agg.trn.csv')\n","SBIC_val_dir = os.path.join(data_dir, 'SBIC/SBIC.v2.agg.dev.csv')\n","SBIC_test_dir = os.path.join(data_dir, 'SBIC/SBIC.v2.agg.tst.csv')"],"metadata":{"id":"i7Ndt_Otu9ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_EDOS = pd.read_csv(EDOS_dir)\n","raw_SBIC_train = pd.read_csv(SBIC_train_dir)\n","raw_SBIC_val = pd.read_csv(SBIC_val_dir)\n","raw_SBIC_test = pd.read_csv(SBIC_test_dir)"],"metadata":{"id":"MIUB6JGlvFfP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Cleaning\n","For fine-tuning GPT models (Causal Language Modeling), we concatenate the hate speech and the counter speech in one string with each begins with their prefix:\n","# **Hate-speech: \"Text\" Counter-speech: \"Text\"**\n","\n","---\n","First, we convert all dataframes into the formate of\n","\n","**{\"Hate_Speech\": hate_speech, \"Counter_Speech\": counter_speech, \"Target\": target}**\n","\n","Then we concatenate the dataframes and perform further normalization steps"],"metadata":{"id":"6Go5iwGJQunn"}},{"cell_type":"markdown","source":["### Qian Benchmark Dataset\n","This dataset contains multiturn conversations and hate speech index, which indecate the hate speech segment inside the conversation, and the corresponding 3 response options.\n","\n","For data cleaning, we first want to seperate the conversations into individual segments, and the responses into individual response.\n","\n","Then, we take the hate-speech and response pair and concatenate them according to our defined data format.\n","\n","Since there can be multiple hate-speech inside one conversation, and the dataset provides multiple responses for each conversation, we use combinatorial pairs to capture all possible combinations:\n","\n",".\n","\n","---\n","\n","currently, this dataset is causing the model to perform in an undesired way. Therefore, we are not including it in our training and testing pipeline.\n","\n","---\n","\n"],"metadata":{"id":"C8uSmk4TQxed"}},{"cell_type":"code","source":["def Qian_preprocess(dataframe):\n","  # Some general preprocessing at first\n","\n","  df = dataframe.copy()\n","\n","  # Remove unwanted information\n","  df = df.drop(\"id\", axis=1)\n","\n","  # drop rows without hate speech\n","  df = df[df['hate_speech_idx'].notna()].reset_index()\n","\n","  # remove square brackets\n","  df['hate_speech_idx'] = df['hate_speech_idx'].str.strip('[]')\n","  df['response'] = df['response'].str.strip('[]')\n","\n","  # drop old index\n","  df = df.drop(\"index\", axis=1)\n","\n","  # split conversation segments into individual strings\n","  df['text'] = df['text'].str.split('\\n')\n","\n","  # split responses into individual strings\n","  df['response'] = df['response'].str.split(',')\n","\n","  # split hate speech index into individual strings\n","  df['hate_speech_idx'] = df['hate_speech_idx'].str.split(',')\n","\n","  return df\n","\n","def Qian_clean(list_):\n","  # Text cleaning function, removes URLs, EMOJIs and extra whitespaces or special characters\n","  for idx, text in enumerate(list_):\n","    # pop element if it's empty (due to spliting with \",\")\n","    if text == \"\":\n","      list_.pop()\n","    else:\n","      # remove the numbering, which are the first three characters\n","      list_[idx] = text[3:]\n","\n","\n","  return list_\n","\n","def Qian_concatenate(df):\n","  # Create new dataframe with format \"Hate-speech: \"hatespeech text\" Counter-speech: \"counterspeech text\", for each hate- and counter-speech pair\n","  n_entry = df.index.stop\n","  data = []\n","\n","  # go through each row in dataframe and check the number of hate speech and responses\n","  for idx in range(n_entry):\n","    # for each hate speech, concatenate each response as a pair\n","    for i, hate_idx in enumerate(df[\"hate_speech_idx\"][idx]):\n","      if len(df[\"text\"][idx]) < int(hate_idx):\n","        # Skip if the hate_speech_idx is wrong (outside of the given conversation, e.g. there're in total 14 segments, but the hate_idx is 20)\n","        pass\n","\n","      else:\n","        hate_speech = df['text'][idx][int(hate_idx) - 1]\n","        for j, response in enumerate(df[\"response\"][idx]):\n","          row = [\"Hate-speech: \" + hate_speech + \" \" + \"Counter-speech: \" + response]\n","          data.append(row)\n","\n","  df_new = pd.DataFrame(data,columns =['text'])\n","  return df_new\n","\n","def Qian_pipeline(df_raw):\n","  df = Qian_preprocess(df_raw)\n","  df[\"text\"].map(Qian_clean)\n","  df = Qian_concatenate(df)\n","\n","  return df"],"metadata":{"id":"UDongpUeQytN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CONAN Dataset\n","This dataset contains single turn conversations with one hate-speech and its corresponding counter-speech in EN, FR and IT. We filter out the english language pairs and simply concatenate them into one to fit our format.\n","\n","\n","\n","---"],"metadata":{"id":"_6GqNSKWQ3S1"}},{"cell_type":"code","source":["def CONAN_format(dataframe, language='EN'):\n","  df = dataframe[dataframe['cn_id'].str.startswith(language)]\n","\n","  # drop old index\n","  df = df.drop(\"cn_id\", axis=1)\n","  df = df.drop(\"cnType\", axis=1)\n","  df = df.drop(\"age\", axis=1)\n","  df = df.drop(\"gender\", axis=1)\n","  df = df.drop(\"educationLevel\", axis=1)\n","  df = df.rename(columns={\"hateSpeech\": \"Hate_Speech\", \"counterSpeech\": \"Counter_Speech\", \"hsType\": \"Target\", \"hsSubType\": \"Target_2\"})\n","\n","  return df"],"metadata":{"id":"u2jXL4X5Q35z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multitarget CONAN Dataset\n","This dataset contains single turn conversations with one hate-speech and its corresponding counter-speech. We simply concatenate the pairs into one to fit our format.\n","\n","\n","---"],"metadata":{"id":"UdMaXLk-Q6b0"}},{"cell_type":"code","source":["def MultiCONAN_format(dataframe):\n","  # Remove unwanted information\n","  df = dataframe.drop(\"VERSION\", axis=1)\n","  df = df.drop(\"INDEX\", axis=1)\n","\n","  df = df.rename(columns={\"HATE_SPEECH\": \"Hate_Speech\", \"COUNTER_NARRATIVE\": \"Counter_Speech\", \"TARGET\": \"Target\"})\n","\n","  return df"],"metadata":{"id":"BaMTDLXlQ7vz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DIALO CONAN Dataset\n","\n","The DIALOCONAN dataset contains multi-turn diaglogues with multiple hate-speech and counter-speech segments. Our application focuses on single turn hate- and counter-speech pairs, to avoid samples with incomplete context information, we only take the first turn of the each diaglogue conversation, since only the first turn is garanteed to contain complete context for themselves.\n","\n","\n","---"],"metadata":{"id":"c06kyX_HQ9if"}},{"cell_type":"code","source":["def DIALO_format(dataframe):\n","  hate_speech = []\n","  counter_speech = []\n","  target = []\n","\n","  switch = 0\n","\n","  max_idx = dataframe[\"dialogue_id\"].max()+1\n","\n","  for i in range(max_idx):\n","    # slice each diaglue as a sub-dataframe\n","    df = dataframe[(dataframe.dialogue_id == i)]\n","\n","    for index, row in df.iterrows():\n","      if row[\"type\"] == \"HS\" and switch == 0:\n","        hate_speech.append(row[\"text\"])\n","        switch = 1\n","      elif row[\"type\"] == \"CN\" and switch == 1:\n","        counter_speech.append(row[\"text\"])\n","        target.append(row[\"TARGET\"])\n","        switch = 0\n","\n","        break\n","\n","  data = {\"Hate_Speech\": hate_speech, \"Counter_Speech\": counter_speech, \"Target\": target}\n","\n","  df_new = pd.DataFrame(data)\n","  return df_new"],"metadata":{"id":"eMlpeJU_Q-lN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EDOS - Explainable Detection of Online Sexism"],"metadata":{"id":"J44KGhU-yVDQ"}},{"cell_type":"code","source":["df_EDOS = raw_EDOS.copy()\n","df_EDOS = df_EDOS[[\"text\",\"label_sexist\"]]\n","df_EDOS = df_EDOS.rename(columns={\"label_sexist\": \"labels\"})\n","df_EDOS_sexist = df_EDOS.loc[df_EDOS['labels'] == \"sexist\"]"],"metadata":{"id":"_3dihZW10NNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SBIC - Social Bias Frame\n"],"metadata":{"id":"S2wX17ivCx82"}},{"cell_type":"code","source":["def split_SBIC(df):\n","  tuple_head=[]\n","  for index,row in df.iterrows():\n","    tuple_temp=[row['post']]\n","    if \"women\" in row['targetMinority']:\n","      tuple_temp.append('sexist')\n","    else:\n","      tuple_temp.append('not sexist')\n","    tuple_head.append(tuple_temp)\n","  return pd.DataFrame(tuple_head,columns=['text','labels'])"],"metadata":{"id":"2QpNwQNIFsAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SBIC_train = split_SBIC(raw_SBIC_train)\n","SBIC_val = split_SBIC(raw_SBIC_val)\n","SBIC_test = split_SBIC(raw_SBIC_test)"],"metadata":{"id":"eepq40gXCxSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Concatenate Dataframes and Text Normalization"],"metadata":{"id":"fyKi2A2CRApN"}},{"cell_type":"code","source":["df_Qian_gab = Qian_pipeline(raw_Qian_gab)\n","df_Qian_reddit = Qian_pipeline(raw_Qian_reddit)\n","\n","df_CONAN = CONAN_format(raw_CONAN)\n","df_MultiCONAN = MultiCONAN_format(raw_MultiCONAN)\n","df_DIALOCONAN = DIALO_format(raw_DIALOCONAN)"],"metadata":{"id":"-LnJSoUfQ1O1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","types = df_MultiCONAN[\"Target\"].unique()\n","\n","df_MultiCONAN_sexist = df_MultiCONAN.copy()\n","for target in types:\n","  df_MultiCONAN_sexist[target] = np.where(df_MultiCONAN_sexist['Target']== target, 1, 0)\n","\n","df_MultiCONAN_sexist_counter=df_MultiCONAN_sexist.drop(\"Hate_Speech\", axis=1)\n","df_MultiCONAN_sexist_counter=df_MultiCONAN_sexist_counter.rename(columns={\"Counter_Speech\":\"text\", \"Target\":\"TARGET\"})"],"metadata":{"id":"r6fCYrHmhizC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_multiclass(item):\n","  # Text cleaning function, removes URLs, EMOJIs and extra whitespaces or special characters\n","  return item.split(\"/\")\n","\n","df_temp = raw_DIALOCONAN.loc[raw_DIALOCONAN['type'] == \"CN\"]\n","types = ['MIGRANTS', 'POC', 'LGBT+', 'MUSLIMS', 'WOMEN', 'JEWS']\n","df_temp = df_temp.copy()\n","df_temp[\"TARGET\"] = df_temp[\"TARGET\"].map(format_multiclass)\n","\n","tuple_head=[]\n","for index,row in df_temp.iterrows():\n","  tuple_temp=[row['text']]\n","  tuple_temp.append(row[\"TARGET\"])\n","  for target in types:\n","    if target in row[\"TARGET\"]:\n","      tuple_temp.append(1)\n","    else:\n","      tuple_temp.append(0)\n","  tuple_head.append(tuple_temp)\n","\n","df_DIALOCONAN_sexist_counter=pd.DataFrame(tuple_head,columns=['text', \"TARGET\"] + (types))"],"metadata":{"id":"PU5ojJmpjDlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate dataframes for better parallelization\n","select_dataframes = ['CONAN', 'Multi_CONAN', 'DIALO_CONAN']\n","\n","df = pd.concat(\n","    [df_CONAN, df_MultiCONAN, df_DIALOCONAN],\n","    keys=select_dataframes,\n","    names=['Dataset', 'Row ID'],\n","    sort=False)"],"metadata":{"id":"NsyWLIbCRBgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate dataframes for better parallelization\n","select_dataframes_sexist = ['DIALO_CONAN_Counterspeech', 'Multi_CONAN_Counterspeech']\n","\n","df_sexist = pd.concat(\n","    [df_DIALOCONAN_sexist_counter, df_MultiCONAN_sexist_counter],\n","    keys=select_dataframes_sexist,\n","    names=['Dataset', 'Row ID'],\n","    sort=False)\n","df_sexist=df_sexist.fillna(0)"],"metadata":{"id":"Un-dO4r77-bo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Cleaning\n","For fine-tuning GPT models (Causal Language Modeling), we concatenate the hate speech and the counter speech in one string with each begins with their prefix:\n","### **Hate-speech: \"Text\" Counter-speech: \"Text\"**\n","\n","---"],"metadata":{"id":"zdd9AKGity4f"}},{"cell_type":"code","source":["import math\n","import preprocessor as p\n","\n","# set options for cleaning text data (convert URLs and Emojis into special tokens $URL$, $EMJ$ )\n","p.set_options(p.OPT.URL, p.OPT.EMOJI)\n","\n","def clean(item):\n","  # Text cleaning function, removes URLs, EMOJIs and extra whitespaces or special characters\n","  item = p.tokenize(item)\n","\n","  return item"],"metadata":{"id":"_iNVIhNqioZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Hate_Speech'] =df['Hate_Speech'].map(clean)\n","df['Counter_Speech']=df['Counter_Speech'].map(clean)\n","df_sexist['text']=df_sexist['text'].map(clean)\n","df_EDOS_sexist[\"text\"] = df_EDOS_sexist[\"text\"].map(clean)"],"metadata":{"id":"NJOldKqVi2mz","executionInfo":{"status":"ok","timestamp":1686075582684,"user_tz":-120,"elapsed":4066,"user":{"displayName":"Chang YenYu","userId":"13353243669334155647"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4249861-5c91-4dda-95e0-a34e1da53920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-ff45419f4c0f>:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_EDOS_sexist[\"text\"] = df_EDOS_sexist[\"text\"].map(clean)\n"]}]},{"cell_type":"code","source":["\n","df_sexist['text']=df_sexist['text'].map(clean)"],"metadata":{"id":"UdDCOdxqtnXo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save dataframe to csv for future usage"],"metadata":{"id":"FaHTkMO9TgZf"}},{"cell_type":"code","source":["# Create Train/Test split\n","from sklearn.model_selection import train_test_split\n","\n","# train, test = train_test_split(df, test_size=0.2)\n","train_sex, test_sex = train_test_split(df_sexist, test_size=0.1)"],"metadata":{"id":"fqsOceKjX3yT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_dir = os.path.join(data_dir, 'Custom/')\n","train_name = \"Sexist_train.csv\"\n","test_name = \"Sexist_test.csv\"\n","\n","os.makedirs(save_dir, exist_ok=True)\n","train_sex.to_csv(save_dir + train_name)\n","test_sex.to_csv(save_dir + test_name)"],"metadata":{"id":"6ASnq3PzHngf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_dir = os.path.join(data_dir, 'Custom/')\n","train_name = \"CONAN_train.csv\"\n","test_name = \"CONAN_test.csv\"\n","\n","os.makedirs(save_dir, exist_ok=True)\n","train.to_csv(save_dir + train_name)\n","test.to_csv(save_dir + test_name)"],"metadata":{"id":"us8ak7yhTfPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_dir = os.path.join(data_dir, 'Custom/')\n","file_name = \"EDOS_sexist.csv\"\n","\n","\n","os.makedirs(save_dir, exist_ok=True)\n","df_EDOS_sexist.to_csv(save_dir + file_name)"],"metadata":{"id":"S-OLk3CU6CNR"},"execution_count":null,"outputs":[]}]}